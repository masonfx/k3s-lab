global:
  domain: vextech.dev

#
# Prometheus Configuration
#
prometheus:
  prometheusSpec:
    replicas: 1
    # Adjust retention and storage size for homelab use
    retention: 15d
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: longhorn # IMPORTANT: Match your StorageClass
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 10Gi
    # Resource limits tuned for a small cluster
    resources:
      requests:
        cpu: 250m
        memory: 1024Mi
      limits:
        cpu: 2
        memory: 3Gi
    # Disable the default ingress for prometheus
    ingress:
      enabled: false

prometheusOperator:
  admissionWebhooks:
    enabled: true
    patch:
      enabled: true
      # This forces the operator to wait for the webhook to be ready
      # which prevents the "secret not found" mounting error.
      priorityClassName: system-cluster-critical
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 200m
      memory: 256Mi

#
# Alertmanager Configuration
#
alertmanager:
  enabled: true
  alertmanagerSpec:
    replicas: 1
    storage:
      volumeClaimTemplate:
        spec:
          storageClassName: longhorn # IMPORTANT: Match your StorageClass
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 1Gi
    resources:
      requests:
        cpu: 100m
        memory: 100Mi
      limits:
        cpu: 200m
        memory: 250Mi
    ingress:
      enabled: false

grafana:
  enabled: true
  replicas: 1
  adminPassword: "admin" # CHANGE THIS default password
  persistence:
    enabled: true
    type: pvc
    storageClassName: longhorn # IMPORTANT: Match your StorageClass
    accessModes: ["ReadWriteOnce"]
    size: 2Gi
  resources:
    requests:
      cpu: 100m
      memory: 150Mi
    limits:
      cpu: 300m
      memory: 300Mi
  # Add a more patient readiness probe to prevent the sidecar race condition
  readinessProbe:
    initialDelaySeconds: 60
    timeoutSeconds: 30
  # Pre-configure Loki datasource
  # additionalDataSources:
  #   - name: Loki
  #     type: loki
  #     access: proxy
  #     url: http://loki.monitoring.svc.cluster.local:3100
  #     jsonData:
  #       maxLines: 1000
  # Pre-load dashboards from the community
  # These are sourced from grafana.com/grafana/dashboards/
  dashboardProviders:
    grafana-com-dashboards:
      apiVersion: 1
      providers:
        - name: 'default'
          orgId: 1
          folder: ''
          type: 'file'
          disableDeletion: false
          editable: true
          options:
            path: /var/lib/grafana/dashboards/default
  dashboards:
    default:
      # Node Exporter Full Dashboard
      node-exporter:
        gnetId: 1860
        revision: 36
        datasource: Prometheus
      # Kubernetes Cluster Overview
      k8s-cluster:
        gnetId: 3119
        revision: 24
        datasource: Prometheus
      # Cilium Dashboard
      cilium:
        gnetId: 10922
        revision: 23
        datasource: Prometheus
  ingress:
    enabled: false

#
# Resource tuning for exporters
#
prometheus-node-exporter:
  resources:
    requests:
      cpu: 50m
      memory: 64Mi
    limits:
      cpu: 100m
      memory: 128Mi
  podDisruptionBudget:
    enabled: true
    minAvailable: 1
kube-state-metrics:
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 200m
      memory: 256Mi

kubelet:
  service:
    enabled: true            # create exactly one Service
    # give it the same label set as the built-in one
    labels:
      kubernetes.io/name: "Kubelet"
  serviceMonitor:
    # make sure we scrape only that service
    selector:
      matchLabels:
        kubernetes.io/name: "Kubelet"
    # tell Prometheus to key on "job", not "service", so label clashes vanish
    jobLabel: "kubernetes.io/name"
